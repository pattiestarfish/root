Types of Machine Learning (ML)
2 main classes of algorithms in ML: Surpervised vs Unsupervised learning
Supervised learning: explitcitly tell program what to expect output to be, and let them learn the rules that produce expected outputs from given inputs
- commonly ex: imagine classification --> shjoshow program thousands of examples of pictures of labels describing them
*data is labeled and program learns to predict output from input data

Unsupervised learning: program analyzes data it encounters and tries to determine its own patterns and group data in meaningful ways
*data is unlabeled and program learns to recognize inherent structure in input data

***Linear regression model***
from sklearn.linear_model import LinearRegression

your_model = LinearRegression()
Fit {
your_model.fit(x_training_data, y_training_data)
}
Predict{
predictions = your_model.predict(your_x_data)
}
.coef, .intercept, .score
--------------------------------
under supervised and unsupervised learning, we can implement further distinguishments as
Classification vs Regression
regression predicts continuous outputs
classification predicts discrete labels, typically uses binary classification
multi level classification requires segmentation, categorization, and sentiment analysis

Normalization: when comparing datasets, feature with larger dataset will dominante (ie comparing houses age vs how many rooms it has)
-normalization makes each datapoint the same scale so it carries the same weight
max-min normalization: takes (value-min)/(max-min), downside does not handle outliers well
z-score normalization(avoids outlier problems, but returns different scale): value-(mean value)/(standard deviation)
 --> variables exactly equal to mean will equal 0, <0 means below average

Training vs Validation vs Test sets
-learning will vary based upon which algorithm is used
linear regression --> line of best fit
K-nearest neighbors: points in training set are points that could be neighbors
N-Fold Cross-Validation: For small datasets, can perform return iteration of cross-validation by performing entire
testing/training process N times and finding the averaging the accuracy
#----------------------------------------------------------------------------------------------------------
KNN classification algorithm
3 steps:
1. Normalize the data
2. Find the K nearest neighbors
3. Classify new point based on neighbors

*main problem of machine learning algorithms: overfitting(due to relying too heavily on training data(data does not always behave the same))
-however, underfitting can occur when k too large (>100), so trends are not accurately represented
-overfitting occurs when your dataset does not have enough neighbors, single outlier can drastically determine outcome
validation accuracy changes as k changes, accuracy is lower due to overfitting

#---------------------------------------------------------------------------------------------------------
Logistic regression- supervised machine learning algorithm that predicts continuous probability on [0,1], classification
spam emails: if predicted probability is <= 0.5, classified as spam >= 0.5 == ham(negative class, label 0)
-logistic regression eliminates limiter in  linear regression (inaccuracy for low and high values)
Log-Odds: logistic regression, similarly to linear reg also multiplies feature coefficients and feature values and adds intercepts,
but instead of returning prediction, returns log-odds(probability of sample belonging to positive class)

Odds of passing = P(event occurring)/P(event not occurring) = 0.7/0.3 = log(2.33333) = ~ 0.847
np.dot() = dot product, parameters vector (features,  coefficients) + intercept
sigmoid curve eq: h(z) = 1/(1 + np.exp(-z))

once classification probability is calculated, need to make decision which class sample belongs to using
classification threshold (<=0.5 vs >0.5)
- may want to increase threshold sensitivity depending on dataset (cancer may want to lower to 0.2-0.3)

#---------
Evaluating machine learning models -- loss/cost functions
- calculates loss for each dat a sample (residual) and averages loss across all samples

#--------------------------------------------------------
Decision Trees- another form of supervised learning models
- identifies patterns within features of data points, splits data multiple times based on diff features
- decision point when to stop splitting tree, if an unlabeled point reaches that leaf, considered a majority label
how do you know which tree sorting method is best?
calculate information gain of splitting data on certain feature- IG measures difference in impurity of data before and after split
-eg. dataset with impurity of 0.5, when split into 3 subgroups can result in 0, 0.375, 0 for IG of 0.125
gini impurity for set of data points- 1 minus percentage of each label squared, 0 for only 1 class

weighted info gain: much higher confidence levels in high sample sizes

Recursive trees: repeat algorithm to find best feature and split data based on that feature until
no additional recursions results in any information gain
-classifying new data: via recursion and tree nodes
- base case is reached when dividing any additional internal node leads to 0 gain

decision tree limiations: aren't always globally optimal; a split may have negative effects further down
- can potentially overfit data, solution to prune the tree
- usually search for tree that will result in largest info gain instantaneously

#------------------------------------------------------
Random forest- forest of decision trees created through bagging process
- bagging randomly selects a subset of total datapoints and splits them into decision trees(at random, may pick multiple of same row)
- can further randomize after initial split by only choosing from subset to further split (rule of thumb is to split among sqrt total sample size)
another solution to minimize overfitting in decision trees
ensemble MI technique
-when new data point is given, given to every decision tree within forest for classification
    - most popular classification is returned

#----------------------------------------------------
Clustering- finds structure in unlabeled data by identifying similar groups/clusters
K-Means Clustering- tries to address how many groups we choose and how to define similarity
- K refers to number of clusters we expect to find, means refers to average distance of data to each cluster(centroid, try to minimize)
iterative approach:
1. place k random centroids for initial clusters
2. assign data samples to nearest centroid
3. update centroids based on newly assigned samples
repeat steps 2 and 3 until achieve convergence, when points don't move between clusters and centroids stabilize

#-----------------------------------------------------
K-Means++
- addresses K-means clustering issue where randomly initializing centroids may result in suboptimal clusters
builds off same 3 steps for K-Means, only adds additional steps to step 1
1.1 1st cluster centroid picked randomly from data points
1.2 for remaining data points, distance from point to nearest cluster centroid calculated
1.3 next cluster centroid picked according to probability proportional to distance of each point to its nearest centroid

#--------------------------------------------------------
Perceptron- ability tosense stimuli and convert sensory inputs into meaningful outputs
- artificial neuron that can make simple decisions
converting inputs and weights to outputs requires 2 steps:
1. find weighted sum (input)(weight) + .... + ____
2. constrain wsum to produce desired output using activation function
- designed to transform weighted sum into desired/constrained output
- many types, one way is sign activation +1 or -1
3. Train model until training error converages/is minimized
- cannot only alter 1 variable, must alter combination of weights
4. Visualizing perceptron's training process
slope = -self.weights[0]/self.weights[1]
intercept = -self.weights[2]/self.weights[1]
5. theoretically, perceptron should predict all labels correctly once they have found a linear classifier/decision boundary(straight line)

Neural networks
- single perceptrons can only predict linearly separable variables (single line)
- require multiple perceptrons known as neural networks to analyze multifeature data

#-------------------------------------------------------------
Minimax algorithm- recursive algorithm optimizing moves for 2 player games
evaluating strength of leaves:
- if player X wins, strenght of leaf = 1, elif O wins leaf = -1, tie=0
- find min or max value depending on player's turn; build from bottom up (leaf-->child-->parent nodes)

*copying elements in Python: since pyth objects are saved in memory, variables point to location in memory
- if 2 variables point to same object (ie new_board = my_board), changing var of 1 object changes value in other
- use new_board = deepcopy(my_board) to store copy into new place in memory
algorithm:
- first check current game state
- then check whether we are maximizing or minimizing function (player's turn)
* for more advanced trees that require higher computational power, utilize hard limits to limit algorithm
or prune the tree to limit useless traversions
- evaluation functions user defined, programmer defined, based on understanding of GAME

Alpha-Beta pruning:
-ignores parts of tree that are dead ends
if parent node has value of 5 and child node has value 8 but we want to minimize, can ignore child
alpha keeps track of min score maximizing player can get, begins at -inf and is constantly updated as min score increases
if alpha >= beta, can ignore node's children